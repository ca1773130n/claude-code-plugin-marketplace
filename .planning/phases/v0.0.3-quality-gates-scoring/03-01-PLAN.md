---
phase: 03-quality-gates-scoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/score-plugin.sh
autonomous: true
verification_level: sanity

must_haves:
  truths:
    - "score-plugin.sh exits 0 for both real plugins (GRD and multi-cli-harness)"
    - "score-plugin.sh --json produces valid JSON parseable by jq"
    - "GRD scores >= 70 (estimated baseline ~72-82 from research)"
    - "multi-cli-harness scores >= 75 (estimated baseline ~74-84 from research)"
    - "Total score equals sum of 5 category scores, each category max 20"
    - "28 scoring rules implemented across 5 categories"
    - "valid-full fixture scores higher than valid-minimal fixture"
    - "Human-readable output shows per-category breakdown with deduction details"
  artifacts:
    - path: "scripts/score-plugin.sh"
      provides: "Quality scoring engine with 28 rules across 5 categories"
      min_lines: 200
  key_links:
    - from: "scripts/score-plugin.sh"
      to: "plugins/*/. claude-plugin/plugin.json"
      via: "jq JSON parsing of manifest"
      pattern: "jq.*plugin.json"
    - from: "scripts/score-plugin.sh"
      to: "plugins/*/agents/"
      via: "filesystem inspection for naming consistency"
      pattern: "find.*agents"
---

<objective>
Create scripts/score-plugin.sh -- the core quality scoring engine that evaluates plugins across 5 categories (Manifest Completeness, Documentation, Structure Integrity, Naming Conventions, Version Hygiene) with 28 rules and a subtractive scoring model.

Purpose: Provide actionable, numeric quality feedback (0-100) for plugin authors, enabling gradual improvement. This is the foundation for PR quality comments and marketplace quality scores.
Output: scripts/score-plugin.sh with --json and human-readable output modes.
Research basis: Subtractive scoring model inspired by Skypack Quality Score and npms.io multi-dimension scoring (Mota et al. 2016).
</objective>

<execution_context>
@${CLAUDE_PLUGIN_ROOT}/workflows/execute-plan.md
@${CLAUDE_PLUGIN_ROOT}/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-quality-gates-scoring/03-RESEARCH.md
@scripts/validate-plugin.sh
@schemas/plugin.schema.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement score-plugin.sh with 5 scoring categories and 28 rules</name>
  <files>scripts/score-plugin.sh</files>
  <action>
    Create scripts/score-plugin.sh as a bash+jq script following the existing project convention
    (see validate-plugin.sh for style reference). The script accepts a plugin directory path and
    optional --json flag.

    Based on the Skypack Quality Score subtractive model:
    - Each category starts at 20 points; deductions are subtracted per failed check
    - Minimum score per category is 0 (never negative)

    **Arguments:**
    - $1: Plugin directory path (e.g., plugins/GRD)
    - --json: Output machine-readable JSON to stdout (default: human-readable summary)
    - --help: Usage information

    **Category 1: Manifest Completeness (20 pts)**
    Read plugin.json from {plugin_dir}/.claude-plugin/plugin.json:
    - Missing `description` field: -4
    - Missing `version` field: -3
    - Missing `author` field: -3
    - Missing `homepage` field: -2
    - Missing `repository` field: -2
    - Missing `license` field: -2
    - Missing `keywords` field: -2
    - Undeclared artifacts: -2 (agents/, commands/, or skills/ directory exists but not referenced in manifest)

    **Category 2: Documentation (20 pts)**
    Check plugin root directory for documentation files:
    - Missing README: -6 (no README.md AND no *README*.md at plugin root)
    - Missing CLAUDE.md: -6 (no CLAUDE.md at plugin root)
    - README too short: -3 (README file found but < 50 lines)
    - Description too short: -3 (manifest description < 20 characters)
    - Missing keywords for discovery: -2 (no keywords array in manifest)

    **Category 3: Structure Integrity (20 pts)**
    Inspect plugin directory structure:
    - Missing .claude-plugin directory: -6
    - Declared file not found: -4 each (file referenced in manifest commands/agents/skills/hooks doesn't exist), max -12 total
    - Undeclared artifact files: -2 (files exist in agents/commands/skills dirs but not declared in manifest)
    - Hook script not executable: -3 (hook .sh file without +x permission)
    - Empty artifact directory: -2 (directory declared or exists but contains no files)

    **Category 4: Naming Conventions (20 pts)**
    Check naming patterns:
    - Plugin name not lowercase-hyphenated: -4 (name field doesn't match ^[a-z][a-z0-9-]*$)
    - Inconsistent agent naming: -4 (agent .md files don't follow a consistent prefix pattern -- check if >50% share a common prefix; if so, consistent)
    - Command dir nonstandard name: -2 (commands in directory named other than `commands/` -- e.g., `workflows/`)
    - Agent file extension wrong: -3 (agent files listed in manifest not .md)
    - Version string format invalid: -3 (version doesn't match semver pattern ^[0-9]+\.[0-9]+\.[0-9]+)
    - Manifest field naming non-standard: -2 (fields use camelCase where the schema expects lowercase -- check for unexpected keys)

    IMPORTANT per Research Pitfall 1: Check naming CONSISTENCY not conformity. Award full points
    if agents follow ANY consistent pattern (all prefixed with plugin name, or all following a
    recognizable theme). Only deduct for genuinely mixed/inconsistent naming.

    **Category 5: Version Hygiene (20 pts)**
    Check version-related quality:
    - No version in manifest: -6
    - Invalid semver format: -4 (version doesn't match ^[0-9]+\.[0-9]+\.[0-9]+)
    - Pre-1.0 version: -2 (major version is 0)
    - VERSION file mismatch: -3 (VERSION file exists but content doesn't match manifest version)
    - No changelog or version history: -3 (no CHANGELOG.md at plugin root)
    - Version contains build metadata: -2 (version has + suffix)

    **Output format (human-readable, default):**
    ```
    Quality Score: {plugin_name} -- {total}/100

    Manifest Completeness    {score}/20  {deduction_details}
    Documentation            {score}/20  {deduction_details}
    Structure Integrity      {score}/20  {deduction_details}
    Naming Conventions       {score}/20  {deduction_details}
    Version Hygiene          {score}/20  {deduction_details}
    ```

    **Output format (--json):**
    ```json
    {
      "plugin": "{name}",
      "total": N,
      "categories": {
        "manifest_completeness": { "score": N, "max": 20, "deductions": [...] },
        "documentation": { "score": N, "max": 20, "deductions": [...] },
        "structure_integrity": { "score": N, "max": 20, "deductions": [...] },
        "naming_conventions": { "score": N, "max": 20, "deductions": [...] },
        "version_hygiene": { "score": N, "max": 20, "deductions": [...] }
      }
    }
    ```

    Script structure:
    - Use one function per category (score_manifest_completeness, score_documentation, etc.)
    - Each function outputs a JSON fragment via jq
    - Main function aggregates all category results
    - Apply floor of 0 per category (prevent negative scores)
    - Include --help with usage, exit codes (0=success, 1=scoring failed, 2=bad args/missing deps)
    - Make executable (chmod +x)

    Per Research Production Consideration: Use jq -c for compact JSON when passing between
    functions, jq . (pretty) only for final --json display output.
  </action>
  <verify>
    # Script exists and is executable
    test -x scripts/score-plugin.sh

    # Human-readable mode works for both plugins
    ./scripts/score-plugin.sh plugins/GRD
    ./scripts/score-plugin.sh plugins/multi-cli-harness

    # JSON mode produces valid JSON
    ./scripts/score-plugin.sh plugins/GRD --json | jq .
    ./scripts/score-plugin.sh plugins/multi-cli-harness --json | jq .

    # JSON has exactly 5 categories, each with max 20
    ./scripts/score-plugin.sh plugins/GRD --json | jq '.categories | keys | length' # expect 5
    ./scripts/score-plugin.sh plugins/GRD --json | jq '[.categories[].max] | all(. == 20)' # expect true

    # Total equals sum of categories
    ./scripts/score-plugin.sh plugins/GRD --json | jq '.total == ([.categories[].score] | add)' # expect true

    # Score targets met
    GRD_SCORE=$(./scripts/score-plugin.sh plugins/GRD --json | jq '.total')
    MCH_SCORE=$(./scripts/score-plugin.sh plugins/multi-cli-harness --json | jq '.total')
    test "$GRD_SCORE" -ge 70
    test "$MCH_SCORE" -ge 75

    (Level 1: Sanity)
  </verify>
  <done>
    scripts/score-plugin.sh exists, is executable, implements 28 rules across 5 categories,
    produces valid JSON in --json mode, human-readable summary by default.
    GRD scores >= 70, multi-cli-harness scores >= 75. Total equals sum of category scores.
    Each category scored 0-20 with subtractive deductions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Validate scoring against fixture plugins and calibrate deduction values</name>
  <files>scripts/score-plugin.sh</files>
  <action>
    Run score-plugin.sh against all test fixture plugins in tests/fixtures/ to verify the
    rubric produces sensible scores:

    1. Score all 4 valid fixture plugins (valid-full, valid-minimal, valid-complex, valid-optional-fields
       or whatever valid fixtures exist under tests/fixtures/)
    2. Score invalid fixture plugins -- script should still produce a score (scoring is not validation;
       even incomplete plugins get a score, just low)
    3. Verify valid-full fixture scores higher than valid-minimal fixture
    4. Verify no fixture produces a negative total or any category below 0

    If GRD or multi-cli-harness fail to meet their targets (GRD >= 70, multi-cli-harness >= 75),
    adjust deduction point values -- not rules. Specifically:
    - If a plugin scores too low, reduce deductions on rules that penalize both plugins equally
      (e.g., reduce "missing homepage" from -2 to -1)
    - Never add plugin-specific exceptions (per Research Anti-Pattern guidance)
    - Document any calibration adjustments as comments in the script

    Run final verification:
    - Both real plugins meet targets
    - valid-full > valid-minimal
    - All scores are 0-100
    - --help flag works
    - Exit code is 2 for missing arguments

    Per Research Common Implementation Trap: Verify script works on the current platform
    (macOS) -- avoid GNU-specific sed/find flags, stick to jq for JSON manipulation.
  </action>
  <verify>
    # Fixture validation
    for fixture in tests/fixtures/valid-*/; do
      ./scripts/score-plugin.sh "$fixture" --json | jq '.total >= 0 and .total <= 100'
    done

    # valid-full scores higher than valid-minimal
    FULL=$(./scripts/score-plugin.sh tests/fixtures/valid-full --json 2>/dev/null | jq '.total')
    MINIMAL=$(./scripts/score-plugin.sh tests/fixtures/valid-minimal --json 2>/dev/null | jq '.total')
    test "$FULL" -gt "$MINIMAL"

    # Real plugin targets
    test "$(./scripts/score-plugin.sh plugins/GRD --json | jq '.total')" -ge 70
    test "$(./scripts/score-plugin.sh plugins/multi-cli-harness --json | jq '.total')" -ge 75

    # Bad args
    ./scripts/score-plugin.sh 2>/dev/null; test $? -eq 2

    (Level 1: Sanity)
  </verify>
  <done>
    Score-plugin.sh calibrated: all fixture plugins produce valid 0-100 scores,
    valid-full > valid-minimal, both real plugins meet success criteria targets,
    error handling works (exit 2 for bad args). Any calibration adjustments documented
    in script comments.
  </done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- score-plugin.sh exists and is executable
- Both real plugins exit 0 and produce scores meeting targets (GRD >= 70, MCH >= 75)
- JSON output is valid and parseable by jq
- 5 categories, each max 20, total = sum
- valid-full fixture > valid-minimal fixture
- All scores in 0-100 range

Level 2 (Proxy): N/A for this plan -- proxy metrics are in Plans 02/03

Level 3 (Deferred):
- Scoring performance with 10+ plugins (Phase 5)
- Rubric tuning after real-world usage feedback
</verification>

<success_criteria>
- scripts/score-plugin.sh is a standalone, executable bash+jq script
- 28 rules implemented across 5 equal-weight categories (20 pts each)
- Subtractive scoring model with per-category floor of 0
- Dual output: human-readable (default) and --json
- GRD scores >= 70, multi-cli-harness scores >= 75
- Fixture plugins produce sensible, ordered scores
- Exit codes: 0 (success), 1 (scoring failure), 2 (bad args/missing deps)
</success_criteria>

<output>
After completion, create `.planning/phases/03-quality-gates-scoring/03-01-SUMMARY.md`
</output>
