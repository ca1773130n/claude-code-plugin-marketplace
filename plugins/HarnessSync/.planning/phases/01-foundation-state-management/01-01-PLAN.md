---
phase: 01-foundation-state-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/__init__.py
  - src/utils/__init__.py
  - src/utils/logger.py
  - src/utils/hashing.py
  - src/utils/paths.py
autonomous: true
verification_level: sanity

must_haves:
  truths:
    - "Logger produces colored output with synced/skipped/error/cleaned counters and summary string"
    - "Logger disables ANSI codes when not a TTY or on Windows CMD (no WT_SESSION)"
    - "hash_file_sha256 returns consistent 16-char hex string for same file content"
    - "hash_file_sha256 uses hashlib.file_digest on Python 3.11+, chunked reading on 3.10"
    - "create_symlink_with_fallback returns (True, 'symlink') on macOS/Linux for valid src/dst"
    - "create_symlink_with_fallback handles existing destination (file, dir, symlink) before creating"
    - "Stale symlink cleanup removes broken symlinks without touching valid ones"
  artifacts:
    - path: "src/__init__.py"
      provides: "Package marker"
    - path: "src/utils/__init__.py"
      provides: "Utils package with public imports"
      contains: "from .logger import Logger"
    - path: "src/utils/logger.py"
      provides: "Colored logger with summary statistics"
      exports: ["Logger"]
    - path: "src/utils/hashing.py"
      provides: "Version-aware SHA256 file hashing"
      exports: ["hash_file_sha256"]
    - path: "src/utils/paths.py"
      provides: "OS-aware symlink creation with fallback chain"
      exports: ["create_symlink_with_fallback", "cleanup_stale_symlinks", "ensure_dir"]
  key_links:
    - from: "src/utils/__init__.py"
      to: "src/utils/logger.py"
      via: "re-export"
      pattern: "from .logger import Logger"
    - from: "src/utils/__init__.py"
      to: "src/utils/hashing.py"
      via: "re-export"
      pattern: "from .hashing import hash_file_sha256"
    - from: "src/utils/__init__.py"
      to: "src/utils/paths.py"
      via: "re-export"
      pattern: "from .paths import"
---

<objective>
Create the foundational utility modules that all subsequent Phase 1 components depend on: colored logger with summary statistics, version-aware SHA256 file hashing, and OS-aware symlink creation with 3-tier fallback chain.

Purpose: These utilities are the leaf-node dependencies for state_manager.py (needs hashing) and source_reader.py (needs logger, paths). Building them first unblocks parallel development of Plans 02 and 03.

Output: src/utils/ package with logger.py, hashing.py, paths.py -- all importable and tested.

Research basis: Architecture patterns from 01-RESEARCH.md (Pattern 1: OS-Aware Symlink, Pattern 2: Version-Aware Hashing, Pattern 4: Colored Logger). Pitfall avoidance for Windows symlink failures (Pitfall 1), hash mismatch on symlinks (Pitfall 2), ANSI on Windows CMD (Pitfall 5).
</objective>

<execution_context>
@.planning/phases/01-foundation-state-management/01-RESEARCH.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@cc2all-sync.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure and implement Logger</name>
  <files>src/__init__.py, src/utils/__init__.py, src/utils/logger.py</files>
  <action>
    Create the src/ package structure, then implement the Logger class by refactoring from cc2all-sync.py (lines 75-109).

    1. Create src/__init__.py as empty package marker.

    2. Create src/utils/__init__.py with re-exports:
       ```python
       from .logger import Logger
       from .hashing import hash_file_sha256
       from .paths import create_symlink_with_fallback, cleanup_stale_symlinks, ensure_dir
       ```
       NOTE: This will fail to import until Tasks 2 and 3 complete. That is expected.

    3. Create src/utils/logger.py by extracting and enhancing the Logger class from cc2all-sync.py:

       Enhancements over existing cc2all Logger:
       - Add `os` import and WT_SESSION detection for Windows CMD (research Pitfall 5):
         `self.use_colors = sys.stdout.isatty() and not (sys.platform == 'win32' and 'WT_SESSION' not in os.environ)`
       - Add `audit_trail` list that records all messages as dicts: `{'level': 'info'|'warn'|'error'|'skip', 'msg': str, 'timestamp': str}`
       - Add `get_audit_trail() -> list[dict]` method that returns the audit trail
       - Add `reset()` method that clears counts and audit trail (for watch mode reuse)
       - Keep existing interface: info(), warn(), error(), skip(), debug(), header(), synced(), cleaned(), summary()
       - Use `from datetime import datetime` for timestamps in audit trail
       - Maintain the COLORS dict, _c() colorize helper, verbose flag, _counts dict exactly as in cc2all

    Avoid:
    - Do NOT import colorama or any external dependency
    - Do NOT use logging module (custom logger is simpler, matches cc2all pattern)
    - Do NOT add print statements outside the Logger class methods
  </action>
  <verify>
    Run from project root (Level 1: Sanity):
    ```bash
    python3 -c "
    from src.utils.logger import Logger
    log = Logger(verbose=True)
    log.info('test info')
    log.warn('test warn')
    log.error('test error')
    log.skip('test skip')
    log.synced()
    log.synced()
    log.cleaned()
    s = log.summary()
    assert '2 synced' in s, f'Expected 2 synced in: {s}'
    assert '1 error' in s, f'Expected 1 error in: {s}'
    assert '1 skipped' in s, f'Expected 1 skipped in: {s}'
    assert '1 cleaned' in s, f'Expected 1 cleaned in: {s}'
    trail = log.get_audit_trail()
    assert len(trail) >= 4, f'Expected 4+ audit entries, got {len(trail)}'
    assert trail[0]['level'] == 'info'
    log.reset()
    assert log.summary() == 'nothing to do'
    print('Logger: ALL CHECKS PASSED')
    "
    ```
  </verify>
  <done>Logger class in src/utils/logger.py with colored output, 4 counters (synced/skipped/error/cleaned), audit trail, reset(), WT_SESSION detection. All verification assertions pass.</done>
</task>

<task type="auto">
  <name>Task 2: Implement version-aware SHA256 file hashing</name>
  <files>src/utils/hashing.py</files>
  <action>
    Create src/utils/hashing.py implementing the hash_file_sha256 function based on 01-RESEARCH.md Pattern 2 (Version-Aware File Hashing).

    Extract and enhance from cc2all-sync.py file_hash() function (line 119-123).

    Implementation:
    1. `hash_file_sha256(file_path: Path) -> str`:
       - Return empty string if file does not exist
       - On Python 3.11+: Use `hashlib.file_digest(f, 'sha256')` for optimized hashing
       - On Python 3.10: Use manual chunked reading with 8192-byte chunks
       - Always open file in binary mode ('rb') to avoid encoding issues (research Pitfall 2)
       - Truncate hexdigest to 16 chars (matching cc2all convention)
       - If file_path is a symlink, resolve it first before hashing to avoid hashing symlink metadata (research Pitfall 2: Hash Mismatch on Symlinked Files)

    2. `hash_content(content: str) -> str`:
       - Hash a string directly (useful for generated content like AGENTS.md)
       - Encode to UTF-8 bytes, SHA256 hash, truncate to 16 chars

    3. Add module docstring explaining version detection logic

    Avoid:
    - Do NOT load entire file with read_bytes() (research anti-pattern: large files cause MemoryError)
    - Do NOT use MD5 or SHA1 (research: SHA256 is standard for integrity)
    - Do NOT store full 64-char hash (research anti-pattern: readability)
  </action>
  <verify>
    Run from project root (Level 1: Sanity):
    ```bash
    python3 -c "
    import tempfile, os, sys
    from pathlib import Path
    from src.utils.hashing import hash_file_sha256, hash_content

    # Test 1: Known content produces consistent hash
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write('hello world')
        f.flush()
        p = Path(f.name)

    h1 = hash_file_sha256(p)
    h2 = hash_file_sha256(p)
    assert h1 == h2, f'Inconsistent hash: {h1} != {h2}'
    assert len(h1) == 16, f'Hash should be 16 chars, got {len(h1)}'
    print(f'Hash of hello world: {h1}')

    # Test 2: Non-existent file returns empty string
    assert hash_file_sha256(Path('/nonexistent/file.txt')) == ''

    # Test 3: hash_content works
    h3 = hash_content('hello world')
    assert len(h3) == 16
    # File hash of 'hello world' should match content hash
    assert h1 == h3, f'File hash {h1} != content hash {h3}'

    # Test 4: Symlink resolution
    link = Path(tempfile.mkdtemp()) / 'link.txt'
    link.symlink_to(p)
    h4 = hash_file_sha256(link)
    assert h4 == h1, f'Symlink hash {h4} != original hash {h1}'

    # Test 5: Version detection
    if sys.version_info >= (3, 11):
        print('Using file_digest (Python 3.11+)')
    else:
        print('Using chunked reading (Python 3.10)')

    os.unlink(f.name)
    link.unlink()
    print('Hashing: ALL CHECKS PASSED')
    "
    ```
  </verify>
  <done>hash_file_sha256 and hash_content functions in src/utils/hashing.py. 16-char truncated SHA256, version-aware (file_digest on 3.11+), symlink-safe (resolves before hashing), handles missing files gracefully. All verification assertions pass.</done>
</task>

<task type="auto">
  <name>Task 3: Implement OS-aware symlink creation with fallback chain</name>
  <files>src/utils/paths.py</files>
  <action>
    Create src/utils/paths.py implementing OS-aware symlink creation based on 01-RESEARCH.md Pattern 1 (OS-Aware Symlink Creation with Fallback Chain).

    Extract and enhance from cc2all-sync.py safe_symlink() function (lines 146-156).

    Functions to implement:

    1. `ensure_dir(path: Path) -> None`:
       - Simple wrapper: `path.mkdir(parents=True, exist_ok=True)`
       - Extracted from cc2all line 115-116

    2. `create_symlink_with_fallback(src: Path, dst: Path) -> tuple[bool, str]`:
       - Returns (success: bool, method: str) where method is 'symlink', 'junction', 'copy', 'skipped', or 'failed: {reason}'
       - Before creating: ensure dst.parent exists via ensure_dir
       - Before creating: handle existing dst:
         - If dst is symlink pointing to same resolved target as src: return (True, 'skipped') -- already correct
         - If dst is symlink (different target): unlink it
         - If dst is directory: shutil.rmtree(dst)
         - If dst is file: dst.unlink()
       - Try 1: Native symlink via dst.symlink_to(src, target_is_directory=src.is_dir())
         - On success: return (True, 'symlink')
         - On OSError or NotImplementedError: fall through
       - Try 2 (Windows dirs only): Junction point via subprocess.run(['mklink', '/J', str(dst), str(src)], check=True, capture_output=True, shell=True)
         - Only attempt if platform.system() == 'Windows' and src.is_dir()
         - On success: return (True, 'junction')
         - On CalledProcessError: fall through
       - Try 3: Copy with marker file
         - If src is dir: shutil.copytree(src, dst)
         - If src is file: shutil.copy2(src, dst)
         - Create marker: dst.parent / f".harnesssync-source-{dst.name}.txt" containing str(src.resolve())
         - Return (True, 'copy')
       - On all failures: return (False, f'failed: {error_message}')

    3. `cleanup_stale_symlinks(directory: Path) -> int`:
       - Return 0 if directory does not exist or is not a directory
       - Iterate with list(directory.iterdir()) to avoid mutation during iteration (research code example)
       - For each item: if is_symlink() and not resolve().exists(), unlink and increment counter
       - Return count of cleaned symlinks
       - Also clean up orphaned .harnesssync-source-*.txt marker files whose corresponding item is gone

    4. `read_json_safe(file_path: Path, default=None) -> dict`:
       - Return default (or {}) if file does not exist
       - Handle json.JSONDecodeError with warning (log line/col details)
       - Handle OSError, UnicodeDecodeError
       - Extracted from cc2all read_json() (lines 126-133) with better error handling per research Pattern

    5. `write_json_safe(file_path: Path, data: dict) -> None`:
       - Ensure parent directory exists
       - Write with json.dumps() + write_text (non-atomic, simple)
       - Note: Atomic writes are in state_manager.py (Plan 02). This is for general JSON writes.
       - Use indent=2, ensure_ascii=False, trailing newline

    Avoid:
    - Do NOT use os.path -- use pathlib exclusively (research anti-pattern)
    - Do NOT catch broad Exception in file operations -- catch specific exceptions (research anti-pattern)
    - Do NOT import Logger here -- paths.py should be dependency-free for maximum reusability
  </action>
  <verify>
    Run from project root (Level 1: Sanity):
    ```bash
    python3 -c "
    import tempfile, json
    from pathlib import Path
    from src.utils.paths import (
        create_symlink_with_fallback,
        cleanup_stale_symlinks,
        ensure_dir,
        read_json_safe,
        write_json_safe,
    )

    # Test 1: ensure_dir creates nested directories
    tmp = Path(tempfile.mkdtemp())
    nested = tmp / 'a' / 'b' / 'c'
    ensure_dir(nested)
    assert nested.is_dir(), 'ensure_dir failed to create nested dirs'

    # Test 2: Symlink creation
    src_file = tmp / 'source.txt'
    src_file.write_text('hello')
    dst_file = tmp / 'link.txt'
    ok, method = create_symlink_with_fallback(src_file, dst_file)
    assert ok and method == 'symlink', f'Expected (True, symlink), got ({ok}, {method})'
    assert dst_file.read_text() == 'hello'

    # Test 3: Symlink to same target returns skipped
    ok2, method2 = create_symlink_with_fallback(src_file, dst_file)
    assert ok2 and method2 == 'skipped', f'Expected skipped, got ({ok2}, {method2})'

    # Test 4: Symlink replaces existing wrong target
    other = tmp / 'other.txt'
    other.write_text('other')
    ok3, method3 = create_symlink_with_fallback(other, dst_file)
    assert ok3 and method3 == 'symlink'
    assert dst_file.read_text() == 'other'

    # Test 5: Cleanup stale symlinks
    stale = tmp / 'stale_link.txt'
    stale.symlink_to(tmp / 'nonexistent.txt')
    count = cleanup_stale_symlinks(tmp)
    assert count >= 1, f'Expected cleanup of stale link, got {count}'
    assert not stale.exists()

    # Test 6: Cleanup on non-existent dir returns 0
    assert cleanup_stale_symlinks(Path('/nonexistent')) == 0

    # Test 7: JSON round-trip
    json_file = tmp / 'test.json'
    write_json_safe(json_file, {'key': 'value', 'num': 42})
    data = read_json_safe(json_file)
    assert data == {'key': 'value', 'num': 42}

    # Test 8: JSON safe read on missing file
    assert read_json_safe(Path('/nonexistent.json')) == {}

    # Test 9: JSON safe read on corrupted file
    bad_json = tmp / 'bad.json'
    bad_json.write_text('{broken json')
    assert read_json_safe(bad_json) == {}

    import shutil
    shutil.rmtree(tmp)
    print('Paths: ALL CHECKS PASSED')
    "
    ```
  </verify>
  <done>src/utils/paths.py with create_symlink_with_fallback (3-tier: symlink/junction/copy), cleanup_stale_symlinks, ensure_dir, read_json_safe, write_json_safe. All 9 verification assertions pass. No external dependencies.</done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- All three verification scripts pass without errors
- src/utils/logger.py: Logger class instantiates, counters work, summary produces correct string, audit trail records entries, reset clears state
- src/utils/hashing.py: hash_file_sha256 produces consistent 16-char hex, handles missing files, resolves symlinks
- src/utils/paths.py: symlink creation works, cleanup removes stale links, JSON round-trips, handles errors gracefully

Level 2 (Proxy): Deferred to Plan 04 integration verification
Level 3 (Deferred): Windows junction fallback, Python 3.10 compatibility (requires CI)
</verification>

<success_criteria>
- All 5 files exist in src/utils/ (including __init__.py files)
- Logger produces colored output with 4 counters and audit trail
- SHA256 hashing is version-aware and symlink-safe
- Symlink creation has 3-tier fallback chain
- All verification scripts pass cleanly
- No external dependencies imported anywhere
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-state-management/01-01-SUMMARY.md`
</output>
