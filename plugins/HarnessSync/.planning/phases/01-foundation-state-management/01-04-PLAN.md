---
phase: 01-foundation-state-management
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - plugin.json
  - README.md
  - src/__init__.py
  - src/utils/__init__.py
  - src/utils/logger.py
  - src/utils/hashing.py
  - src/utils/paths.py
  - src/state_manager.py
  - src/source_reader.py
autonomous: true
verification_level: sanity

must_haves:
  truths:
    - "plugin.json is valid JSON with name, version, description, hooks, skills, and commands fields"
    - "Zero references to 'cc2all' exist in any src/ file or plugin.json"
    - "All src/ modules import correctly: from src.utils import Logger, hash_file_sha256, create_symlink_with_fallback"
    - "StateManager and SourceReader can be instantiated together without import errors"
    - "Full integration test passes: SourceReader discovers config, hashes computed, StateManager records sync, drift detected on change"
  artifacts:
    - path: "plugin.json"
      provides: "Claude Code plugin manifest"
      contains: "HarnessSync"
    - path: "README.md"
      provides: "Project README with HarnessSync branding"
      contains: "HarnessSync"
  key_links:
    - from: "src/state_manager.py"
      to: "src/source_reader.py"
      via: "integration through caller (not direct import)"
      pattern: "SourceReader.*StateManager"
    - from: "plugin.json"
      to: "src/"
      via: "plugin entry points"
      pattern: "hooks|skills|commands"
---

<objective>
Create the plugin.json manifest (CORE-01), rename all cc2all references to HarnessSync (CORE-05), and run a full integration smoke test proving all Phase 1 modules work together.

Purpose: This final plan completes the two remaining requirements (CORE-01, CORE-05) and validates the entire Phase 1 foundation works as an integrated system. The plugin.json is the entry point for Claude Code to discover the plugin. The rename is a prerequisite before any user-facing code ships.

Output: Valid plugin.json, updated README.md, zero cc2all references in src/, passing integration test.
</objective>

<execution_context>
@.planning/phases/01-foundation-state-management/01-RESEARCH.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation-state-management/01-01-SUMMARY.md
@.planning/phases/01-foundation-state-management/01-02-SUMMARY.md
@.planning/phases/01-foundation-state-management/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create plugin.json and rename cc2all to HarnessSync</name>
  <files>plugin.json, README.md, src/utils/logger.py, src/utils/hashing.py, src/utils/paths.py, src/state_manager.py, src/source_reader.py, src/__init__.py, src/utils/__init__.py</files>
  <action>
    Two deliverables in this task: plugin.json manifest (CORE-01) and cc2all rename (CORE-05).

    **Part A: Create plugin.json (CORE-01)**

    Create plugin.json at the project root with the Claude Code plugin manifest structure.
    Based on 01-RESEARCH.md Plugin Structure section:

    ```json
    {
      "name": "HarnessSync",
      "version": "1.0.0",
      "description": "Sync Claude Code configuration to Codex, Gemini CLI, and OpenCode. Configure once, sync everywhere.",
      "author": "HarnessSync Contributors",
      "license": "MIT",
      "main": "src/",
      "hooks": {
        "PostToolUse": {
          "script": "src/hooks/post_tool_use.py",
          "triggers": ["Write", "Edit", "MultiEdit"],
          "description": "Auto-sync when Claude Code writes to config files"
        }
      },
      "skills": [],
      "commands": {
        "sync": {
          "description": "Sync Claude Code config to all targets",
          "script": "src/commands/sync.py"
        },
        "sync-status": {
          "description": "Show sync status and drift detection",
          "script": "src/commands/sync_status.py"
        }
      },
      "mcp": {
        "server": "src/mcp/server.py",
        "tools": ["sync_all", "sync_target", "get_status"]
      }
    }
    ```

    NOTE: The hooks/commands/mcp scripts referenced do NOT exist yet -- they are Phase 4-6 deliverables. plugin.json declares the full intended structure. This is standard for plugin manifests.

    **Part B: Rename cc2all to HarnessSync (CORE-05)**

    Search ALL src/ files for any remaining "cc2all" references and replace:
    - "cc2all" -> "HarnessSync" (in comments, docstrings, string literals)
    - "cc2all-sync" -> "harnesssync" (in identifiers)
    - "~/.cc2all" -> "~/.harnesssync" (in path references)
    - "cc2all MCP start" -> "HarnessSync MCP start" (in marker comments -- though these are adapter-phase concerns, ensure no references leak)

    The state_manager.py should already use ~/.harnesssync/ (from Plan 02). Verify this is consistent.

    Also scan and update:
    - src/utils/logger.py: Check for any cc2all string in output messages
    - src/utils/hashing.py: Check module docstring
    - src/utils/paths.py: Check for .harnesssync-source marker files (should already use harnesssync)
    - src/state_manager.py: Verify state_dir is ~/.harnesssync, migrate_from_cc2all references old path
    - src/source_reader.py: Check any string references

    **Part C: Update README.md**

    Update the existing README.md at project root:
    - Replace any cc2all references with HarnessSync
    - Ensure project name is "HarnessSync" throughout
    - Keep it brief -- this is a working project README, not documentation

    Avoid:
    - Do NOT modify cc2all-sync.py (it stays as-is for reference, will be removed later)
    - Do NOT rename import paths (they are already src.*)
    - Do NOT create the hooks/commands/mcp scripts (those are future phases)
  </action>
  <verify>
    Run from project root (Level 1: Sanity):
    ```bash
    # Check 1: plugin.json exists and is valid JSON
    python3 -c "
    import json
    from pathlib import Path
    pj = json.loads(Path('plugin.json').read_text())
    assert pj['name'] == 'HarnessSync'
    assert 'hooks' in pj
    assert 'commands' in pj
    print('plugin.json: VALID')
    "

    # Check 2: Zero cc2all references in src/
    python3 -c "
    from pathlib import Path
    violations = []
    for f in Path('src').rglob('*.py'):
        content = f.read_text()
        for i, line in enumerate(content.split('\n'), 1):
            if 'cc2all' in line.lower() and 'migrate_from_cc2all' not in line and 'old cc2all' not in line.lower() and '_cc2all' not in line:
                violations.append(f'{f}:{i}: {line.strip()}')
    if violations:
        print('VIOLATIONS FOUND:')
        for v in violations:
            print(f'  {v}')
        # Allow references in migration code (these are intentional)
        real_violations = [v for v in violations if 'migrate' not in v.lower() and 'legacy' not in v.lower() and 'old' not in v.lower()]
        assert len(real_violations) == 0, f'{len(real_violations)} real cc2all references remain'
    print('Rename check: PASSED (zero non-migration cc2all references)')
    "

    # Check 3: README references HarnessSync
    python3 -c "
    content = open('README.md').read()
    assert 'HarnessSync' in content, 'README missing HarnessSync'
    print('README: PASSED')
    "
    ```
  </verify>
  <done>plugin.json created with full manifest structure (name, hooks, commands, mcp declarations). All src/ files use "HarnessSync" naming with zero non-migration cc2all references. README.md updated with HarnessSync branding.</done>
</task>

<task type="auto">
  <name>Task 2: Run full integration smoke test across all Phase 1 modules</name>
  <files></files>
  <action>
    Create and run an integration test that proves all Phase 1 modules work together as a coherent system. This does NOT create new source files -- it tests the existing ones.

    The integration test should verify the full pipeline:
    1. SourceReader discovers config from a mock project
    2. Hashing computes SHA256 for discovered files
    3. Logger tracks operations with colored output
    4. StateManager records the sync with file hashes
    5. Modifying a file and re-hashing shows drift
    6. StateManager.detect_drift identifies the changed file
    7. Symlink creation works for skill directories

    Run this as a single python3 -c verification command. The test should:
    - Create a temporary mock project with all 6 config types
    - Instantiate SourceReader, discover all configs
    - Hash each discovered source file
    - Create symlinks for skills using create_symlink_with_fallback
    - Record sync via StateManager
    - Modify one config file
    - Re-hash and detect drift
    - Verify Logger summary output is correct
    - Clean up temp directory

    This integration test validates the key_links between modules:
    - SourceReader -> paths.py (read_json_safe)
    - StateManager -> hashing.py (hash_file_sha256)
    - StateManager -> paths.py (ensure_dir, read_json_safe)
    - All modules -> Logger for operation tracking

    If the integration test passes, Phase 1 foundation is solid and Phase 2 (adapters) can build on it confidently.

    Avoid:
    - Do NOT write test files to the src/ directory
    - Do NOT modify any production code in this task
    - Do NOT test against real user ~/.claude/ directory
  </action>
  <verify>
    Run from project root (Level 1: Sanity -- full integration):
    ```bash
    python3 -c "
    import tempfile, json
    from pathlib import Path

    # Import all Phase 1 modules
    from src.utils.logger import Logger
    from src.utils.hashing import hash_file_sha256, hash_content
    from src.utils.paths import create_symlink_with_fallback, cleanup_stale_symlinks, ensure_dir, read_json_safe
    from src.state_manager import StateManager
    from src.source_reader import SourceReader

    print('=== Phase 1 Integration Test ===')
    print()

    # Setup: Create mock project
    tmp = Path(tempfile.mkdtemp())
    project = tmp / 'integration-test-project'
    project.mkdir()
    claude_dir = project / '.claude'
    claude_dir.mkdir()

    # Create all 6 config types
    (project / 'CLAUDE.md').write_text('# Integration Test Rules\nAlways test thoroughly.')
    skill_dir = claude_dir / 'skills' / 'test-skill'
    skill_dir.mkdir(parents=True)
    (skill_dir / 'SKILL.md').write_text('---\nname: test-skill\n---\nTest skill content.')
    agents_dir = claude_dir / 'agents'
    agents_dir.mkdir()
    (agents_dir / 'test-agent.md').write_text('You are a test agent.')
    commands_dir = claude_dir / 'commands'
    commands_dir.mkdir()
    (commands_dir / 'test-cmd.md').write_text('Run test command.')
    (project / '.mcp.json').write_text(json.dumps({'mcpServers': {'test-server': {'command': 'node', 'args': ['server.js']}}}))
    (claude_dir / 'settings.json').write_text(json.dumps({'allowedTools': ['bash']}))

    # Step 1: SourceReader discovers all config
    reader = SourceReader(scope='project', project_dir=project)
    all_config = reader.discover_all()
    assert all_config['rules'] != '', 'Rules should not be empty'
    assert 'test-skill' in all_config['skills']
    assert 'test-agent' in all_config['agents']
    assert 'test-cmd' in all_config['commands']
    assert 'test-server' in all_config['mcp_servers']
    assert 'allowedTools' in all_config['settings']
    print('Step 1 PASSED: SourceReader discovers all 6 config types')

    # Step 2: Hash discovered source files
    source_paths = reader.get_source_paths()
    file_hashes = {}
    for config_type, paths in source_paths.items():
        for p in paths:
            if p.is_file():
                h = hash_file_sha256(p)
                assert len(h) == 16, f'Hash wrong length for {p}'
                file_hashes[str(p)] = h
    assert len(file_hashes) >= 4, f'Expected 4+ hashed files, got {len(file_hashes)}'
    print(f'Step 2 PASSED: Hashed {len(file_hashes)} source files')

    # Step 3: Create symlinks for skills
    target_skills = tmp / 'target' / 'skills'
    for name, skill_path in all_config['skills'].items():
        dst = target_skills / name
        ok, method = create_symlink_with_fallback(skill_path, dst)
        assert ok, f'Symlink failed for {name}: {method}'
    assert (target_skills / 'test-skill').is_symlink()
    print('Step 3 PASSED: Symlinks created for skills')

    # Step 4: Logger tracks operations
    log = Logger(verbose=True)
    log.info('Synced rules')
    log.synced()
    log.info('Synced skill: test-skill')
    log.synced()
    log.skip('No changes to agents')
    summary = log.summary()
    assert '2 synced' in summary
    assert '1 skipped' in summary
    trail = log.get_audit_trail()
    assert len(trail) >= 2
    print(f'Step 4 PASSED: Logger summary: {summary}')

    # Step 5: StateManager records sync
    state_dir = tmp / 'state'
    sm = StateManager(state_dir=state_dir)
    sm.record_sync(
        target='codex',
        scope='project',
        file_hashes=file_hashes,
        sync_methods={str(target_skills / 'test-skill'): 'symlink'},
        synced=2, skipped=1, failed=0
    )
    assert sm.last_sync is not None
    status = sm.get_target_status('codex')
    assert status['status'] == 'success'
    assert status['items_synced'] == 2
    print('Step 5 PASSED: StateManager recorded sync')

    # Step 6: Modify a file to create drift
    rules_file = project / 'CLAUDE.md'
    rules_file.write_text('# MODIFIED RULES\nThis was changed after sync.')
    new_hashes = {}
    for path_str, old_hash in file_hashes.items():
        p = Path(path_str)
        if p.exists():
            new_hashes[path_str] = hash_file_sha256(p)
    # Verify the rules file hash actually changed
    rules_path = str(project / 'CLAUDE.md')
    if rules_path in file_hashes and rules_path in new_hashes:
        assert file_hashes[rules_path] != new_hashes[rules_path], 'Hash should change after modification'
    print('Step 6 PASSED: File modification detected via hash change')

    # Step 7: Detect drift
    drifted = sm.detect_drift('codex', new_hashes)
    assert len(drifted) >= 1, f'Expected drift, got {len(drifted)} drifted files'
    print(f'Step 7 PASSED: Drift detected in {len(drifted)} file(s)')

    # Step 8: Cleanup stale symlinks
    stale = target_skills / 'dead-link'
    stale.symlink_to(tmp / 'nonexistent-target')
    cleaned = cleanup_stale_symlinks(target_skills)
    assert cleaned >= 1
    print(f'Step 8 PASSED: Cleaned {cleaned} stale symlink(s)')

    # Step 9: Verify all imports work from package
    from src.utils import Logger as L, hash_file_sha256 as H, create_symlink_with_fallback as S, ensure_dir as E
    print('Step 9 PASSED: Package imports work')

    # Cleanup
    import shutil
    shutil.rmtree(tmp)

    print()
    print('=== Phase 1 Integration Test: ALL 9 STEPS PASSED ===')
    print('Foundation is solid. Ready for Phase 2 (Adapter Framework).')
    "
    ```
  </verify>
  <done>Full integration test passes: SourceReader discovers 6 config types, hashing computes SHA256 for files, symlinks created via fallback chain, Logger tracks operations with summary, StateManager records sync and detects drift on file modification, stale symlink cleanup works, package imports resolve. All 9 integration steps pass -- Phase 1 foundation is validated.</done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- plugin.json is valid JSON with correct structure
- Zero cc2all references in src/ (except migration code)
- README.md references HarnessSync
- Full integration test passes all 9 steps

Level 2 (Proxy): Integration test serves as proxy -- exercises the real pipeline with mock data
Level 3 (Deferred): Real-world test against actual ~/.claude/ configs, Windows compatibility
</verification>

<success_criteria>
- plugin.json exists with HarnessSync manifest (name, hooks, commands, mcp)
- Zero non-migration cc2all references in src/
- README.md branded as HarnessSync
- Integration test proves: SourceReader -> hash -> symlink -> Logger -> StateManager -> drift detection pipeline works end-to-end
- All Phase 1 requirements (CORE-01 through CORE-05, SRC-01 through SRC-06) are covered
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-state-management/01-04-SUMMARY.md`
</output>
