---
phase: 06-mcp-server-integration
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - src/mcp/tools.py
  - src/mcp/server.py
autonomous: true
verification_level: proxy

must_haves:
  truths:
    - "sync_all tool invokes SyncOrchestrator.sync_all() and returns structured JSON results with per-target sync counts and errors"
    - "sync_target tool invokes SyncOrchestrator.sync_all() filtered to single target and returns structured JSON results"
    - "get_status tool invokes SyncOrchestrator.get_status() and returns structured JSON with last_sync, drift, file counts"
    - "get_status executes immediately without queuing (no sync lock needed)"
    - "Server serializes sync operations via worker thread and request queue — only one sync runs at a time"
    - "When sync is already in progress, new sync requests return immediate 'sync in progress' response (not queued, not blocked)"
    - "Server completes full MCP handshake (initialize -> initialized notification -> tools/list -> tools/call)"
    - "Tool execution errors return JSON-RPC success with isError=true in result (not protocol-level errors)"
    - "Server exits cleanly on stdin EOF (client disconnect)"
    - "sync_target('codex') responds within 5 seconds for typical project (success criteria #3)"
  artifacts:
    - path: "src/mcp/tools.py"
      provides: "Tool handler functions bridging MCP to SyncOrchestrator"
      min_lines: 80
    - path: "src/mcp/server.py"
      provides: "Main MCP server entry point with worker thread concurrency"
      min_lines: 100
  key_links:
    - from: "src/mcp/tools.py"
      to: "src/orchestrator.py"
      via: "import and instantiation of SyncOrchestrator"
      pattern: "from.*orchestrator.*import.*SyncOrchestrator"
    - from: "src/mcp/tools.py"
      to: "src/mcp/schemas.py"
      via: "import validators for input validation"
      pattern: "from.*schemas.*import.*VALIDATORS"
    - from: "src/mcp/server.py"
      to: "src/mcp/protocol.py"
      via: "import ProtocolHandler for message routing"
      pattern: "from.*protocol.*import.*ProtocolHandler"
    - from: "src/mcp/server.py"
      to: "src/mcp/transport.py"
      via: "import StdioTransport for I/O"
      pattern: "from.*transport.*import.*StdioTransport"
    - from: "src/mcp/server.py"
      to: "src/mcp/tools.py"
      via: "import tool handlers for tools/call dispatch"
      pattern: "from.*tools.*import"
    - from: "src/mcp/server.py"
      to: "src/lock.py"
      via: "import sync_lock for file-level concurrency"
      pattern: "from.*lock.*import.*sync_lock"
---

<objective>
Wire MCP tool handlers to SyncOrchestrator and implement the main server with worker-thread concurrency for serialized sync operations.

Purpose: Complete the MCP server so it can be launched via `python src/mcp/server.py` and respond to sync_all, sync_target, and get_status tool calls from any MCP client. This fulfills MCP-01, MCP-02, and all four success criteria.

Output: Two files — tool handler bridge (tools.py) and server entry point (server.py) with concurrent request handling.
</objective>

<execution_context>
@${CLAUDE_PLUGIN_ROOT}/references/execute-plan.md
@${CLAUDE_PLUGIN_ROOT}/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-mcp-server-integration/06-RESEARCH.md
@.planning/phases/06-mcp-server-integration/06-01-SUMMARY.md
@plugin.json
@src/orchestrator.py
@src/lock.py
@src/mcp/protocol.py
@src/mcp/schemas.py
@src/mcp/transport.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement tool handler bridge between MCP and SyncOrchestrator</name>
  <files>src/mcp/tools.py</files>
  <action>
    **src/mcp/tools.py — ToolHandlers class:**

    Bridge layer between MCP protocol and SyncOrchestrator. Each handler validates input, invokes orchestrator, and formats MCP tool result.

    Imports:
    - `import json, os, logging, sys` from stdlib
    - `from pathlib import Path`
    - Configure `logger = logging.getLogger(__name__)` (inherits stderr from server)

    **Deferred imports pattern:** Import SyncOrchestrator and related modules inside handler methods (not at module level) to match existing project convention (see hooks/PostToolUse pattern — Decision #38). This avoids import errors if orchestrator dependencies are missing.

    **Class ToolHandlers:**

    `__init__(self, project_dir: Path = None)`:
    - If `project_dir` is None, derive from `os.environ.get("CLAUDE_PROJECT_DIR", os.getcwd())`
    - Store `self.project_dir`

    `handle_sync_all(self, arguments: dict) -> dict`:
    - Import validators: `from src.mcp.schemas import VALIDATORS`
    - Validate: `params = VALIDATORS["sync_all"](arguments)`
    - Deferred import: `from src.orchestrator import SyncOrchestrator`
    - Create orchestrator: `SyncOrchestrator(project_dir=self.project_dir, scope="all", dry_run=params["dry_run"], allow_secrets=params["allow_secrets"])`
    - Call `orchestrator.sync_all()` wrapped in try/except
    - Format results into structured JSON dict:
      ```python
      {
          "status": "success" | "blocked" | "error",
          "targets": {
              "codex": {"synced": N, "skipped": N, "failed": N, "errors": [...]},
              "gemini": {...},
              "opencode": {...}
          },
          "blocked_reason": None | "secrets_detected",
          "warnings": [],
          "compatibility_report": None | "..."
      }
      ```
    - Extract per-target counts from SyncResult objects (same logic as /sync command's format_results_table)
    - Handle `_blocked`, `_warnings`, `_conflicts`, `_compatibility_report` special keys from orchestrator results
    - Return MCP tool result: `{"content": [{"type": "text", "text": json.dumps(structured_result, indent=2)}], "isError": False}`
    - On exception: `{"content": [{"type": "text", "text": f"Sync failed: {type(e).__name__}: {str(e)}"}], "isError": True}`

    `handle_sync_target(self, arguments: dict) -> dict`:
    - Validate: `params = VALIDATORS["sync_target"](arguments)`
    - Same as sync_all but create orchestrator with `scope="all"` (orchestrator always syncs all, we filter results to single target)
    - After `orchestrator.sync_all()`, extract only the results for `params["target"]`
    - Format structured JSON with only the requested target's data
    - Same error handling as sync_all

    `handle_get_status(self, arguments: dict) -> dict`:
    - Validate: `params = VALIDATORS["get_status"](arguments)`
    - Deferred import: `from src.orchestrator import SyncOrchestrator`
    - Create orchestrator: `SyncOrchestrator(project_dir=self.project_dir, scope="all")`
    - Call `orchestrator.get_status()`
    - Return the status dict as JSON text in MCP tool result
    - On exception: return isError=True result

    **Module-level dispatch dict:**
    ```python
    TOOL_HANDLERS = {
        "sync_all": "handle_sync_all",
        "sync_target": "handle_sync_target",
        "get_status": "handle_get_status"
    }
    ```

    **IMPORTANT: Two error levels (see research Pitfall 4):**
    - Validation errors (ValueError from schema validators) → return `isError: True` tool result (NOT JSON-RPC error)
    - Orchestrator exceptions → return `isError: True` tool result
    - Only unknown tool names → JSON-RPC -32601 error (handled in protocol.py, not here)
  </action>
  <verify>
    Verify in Python REPL (with CLAUDE_PROJECT_DIR set to a test directory):
    1. `from src.mcp.tools import ToolHandlers, TOOL_HANDLERS` — imports without error
    2. `len(TOOL_HANDLERS) == 3` — three tool handlers registered
    3. `th = ToolHandlers(Path("/tmp/test-project")); r = th.handle_get_status({})` — returns dict with "content" key
    4. `r = th.handle_sync_target({"target": "invalid"})` — returns dict with `isError: True` (validation error, not crash)
    5. `r = th.handle_sync_all(None)` — returns dict (None args handled gracefully)
    (Level 1: Sanity)
  </verify>
  <done>
    ToolHandlers bridges MCP to SyncOrchestrator with deferred imports, input validation, structured JSON result formatting, and proper two-level error handling (tool errors vs protocol errors).
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement MCP server with worker thread concurrency</name>
  <files>src/mcp/server.py</files>
  <action>
    **src/mcp/server.py — Main MCP server entry point:**

    This is the file referenced in `plugin.json` as `"mcp": {"server": "src/mcp/server.py"}`. Claude Code will launch it via `python src/mcp/server.py`.

    **Imports:**
    - `import sys, os, json, logging, threading, queue` from stdlib
    - `from pathlib import Path`

    **CRITICAL first lines (before ANY other code):**
    ```python
    # Configure ALL logging to stderr — stdout is the JSON-RPC protocol channel
    logging.basicConfig(
        stream=sys.stderr,
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
    )
    logger = logging.getLogger("harnesssync.mcp")
    ```

    **Path setup (same pattern as src/commands/sync.py):**
    ```python
    PLUGIN_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.insert(0, PLUGIN_ROOT)
    ```

    **Then import MCP modules:**
    ```python
    from src.mcp.transport import StdioTransport
    from src.mcp.protocol import ProtocolHandler
    from src.mcp.tools import ToolHandlers, TOOL_HANDLERS
    from src.mcp.schemas import VALIDATORS
    ```

    **Class MCPServer:**

    `__init__(self)`:
    - `self.transport = StdioTransport()`
    - `self.protocol = ProtocolHandler(self.transport)`
    - `self.tool_handlers = ToolHandlers()`
    - `self.sync_queue = queue.Queue(maxsize=1)` — maxsize=1 means at most 1 pending sync request
    - `self.sync_in_progress = False` — flag for immediate status check
    - `self.response_map = {}` — dict mapping request_id -> result (for async delivery)
    - `self.response_lock = threading.Lock()` — protect response_map
    - Start worker thread: `self.worker = threading.Thread(target=self._sync_worker, daemon=True); self.worker.start()`

    `_sync_worker(self)`:
    - Loop forever: `request_id, tool_name, arguments = self.sync_queue.get()`
    - Set `self.sync_in_progress = True`
    - Use existing `sync_lock` from `src.lock` for file-level concurrency:
      ```python
      from src.lock import sync_lock
      try:
          with sync_lock():
              handler_method = getattr(self.tool_handlers, TOOL_HANDLERS[tool_name])
              result = handler_method(arguments)
      except BlockingIOError:
          result = {"content": [{"type": "text", "text": "Another sync operation is in progress (file lock held by /sync command or hook)"}], "isError": True}
      except Exception as e:
          logger.exception("Sync worker error")
          result = {"content": [{"type": "text", "text": f"Internal error: {type(e).__name__}"}], "isError": True}
      ```
    - Set `self.sync_in_progress = False`
    - Build JSON-RPC response: `response = self.protocol.make_success(request_id, result)`
    - Write via transport: `self.transport.write_response(response)`
    - Call `self.sync_queue.task_done()`

    `_handle_tools_call(self, request_id, params: dict)`:
    - Extract `tool_name = params.get("name")` and `arguments = params.get("arguments", {})`
    - If `tool_name not in TOOL_HANDLERS`: return `self.protocol.make_error(request_id, -32602, f"Unknown tool: {tool_name}")`
    - **Validate arguments early** (before queueing): wrap `VALIDATORS[tool_name](arguments)` in try/except ValueError → return `self.protocol.make_success(request_id, {"content": [{"type": "text", "text": str(e)}], "isError": True})`
    - If `tool_name == "get_status"`: Execute immediately (no queue), call `self.tool_handlers.handle_get_status(arguments)`, return success response directly
    - If `tool_name in ["sync_all", "sync_target"]`:
      - If `self.sync_in_progress`: return immediate response with `{"content": [{"type": "text", "text": json.dumps({"status": "busy", "message": "Sync operation already in progress. Use get_status to check current state."})}], "isError": False}`
      - Try `self.sync_queue.put_nowait((request_id, tool_name, arguments))` — if `queue.Full`, return "busy" response
      - Return `None` (response will be sent by worker thread)

    `run(self)`:
    - `logger.info("HarnessSync MCP server started")`
    - Main loop: `while True:`
      - `message = self.transport.read_message()`
      - If `message is None`: break (EOF, client disconnected)
      - Route through protocol: `response = self.protocol.handle_message(message)`
      - Special case: if `message.method == "tools/call"`, call `self._handle_tools_call()` instead of protocol's generic handler
      - If response is not None, write via transport: `self.transport.write_response(response)`
    - `logger.info("HarnessSync MCP server stopped")`

    **Entry point:**
    ```python
    if __name__ == "__main__":
        server = MCPServer()
        try:
            server.run()
        except KeyboardInterrupt:
            logger.info("Server interrupted")
        except Exception as e:
            logger.exception("Server fatal error")
            sys.exit(1)
    ```

    **IMPORTANT implementation notes:**
    - The worker thread sends responses directly via transport (not through the main loop) — this is safe because only one sync runs at a time and get_status is handled in the main thread
    - Use `daemon=True` for worker thread so it dies when main thread exits (graceful shutdown per research recommendation)
    - `sync_lock()` from src/lock.py provides file-level mutual exclusion with /sync command and PostToolUse hook — this is the EXISTING concurrency mechanism (Decision #37)
    - Queue maxsize=1 prevents unbounded memory growth (research Production Considerations)
    - Response writing from worker thread is safe because main thread only writes non-tools/call responses
  </action>
  <verify>
    **Level 1 (Sanity) — Module import and structure:**
    1. `python -c "from src.mcp.server import MCPServer; print('OK')"` — imports without error
    2. Verify server.py has no `print()` calls (only sys.stdout.write via transport)
    3. Verify logging configured to stderr

    **Level 2 (Proxy) — End-to-end MCP protocol test:**
    Create a test script that pipes JSON-RPC messages to the server via subprocess:

    ```python
    import subprocess, json, time

    # Start server
    proc = subprocess.Popen(
        ["python", "src/mcp/server.py"],
        stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
        cwd="/path/to/project"
    )

    def send_recv(msg):
        proc.stdin.write((json.dumps(msg) + "\n").encode())
        proc.stdin.flush()
        line = proc.stdout.readline()
        return json.loads(line) if line else None

    # Test 1: Initialize handshake
    r = send_recv({"jsonrpc": "2.0", "method": "initialize", "id": 1, "params": {"protocolVersion": "2024-11-05", "capabilities": {}, "clientInfo": {"name": "test"}}})
    assert r["result"]["capabilities"]["tools"] is not None, "Missing tools capability"

    # Send initialized notification (no response expected)
    proc.stdin.write((json.dumps({"jsonrpc": "2.0", "method": "initialized"}) + "\n").encode())
    proc.stdin.flush()

    # Test 2: Tools list
    r = send_recv({"jsonrpc": "2.0", "method": "tools/list", "id": 2, "params": {}})
    assert len(r["result"]["tools"]) == 3, f"Expected 3 tools, got {len(r['result']['tools'])}"
    tool_names = {t["name"] for t in r["result"]["tools"]}
    assert tool_names == {"sync_all", "sync_target", "get_status"}, f"Wrong tools: {tool_names}"

    # Test 3: get_status (immediate, no queue)
    r = send_recv({"jsonrpc": "2.0", "method": "tools/call", "id": 3, "params": {"name": "get_status", "arguments": {}}})
    assert "result" in r and "content" in r["result"], "get_status failed"
    assert r["result"]["isError"] is False, "get_status returned error"

    # Test 4: sync_target with invalid target (validation error)
    r = send_recv({"jsonrpc": "2.0", "method": "tools/call", "id": 4, "params": {"name": "sync_target", "arguments": {"target": "invalid"}}})
    assert r["result"]["isError"] is True, "Should be validation error"

    # Test 5: sync_target with valid target (queued, async response)
    proc.stdin.write((json.dumps({"jsonrpc": "2.0", "method": "tools/call", "id": 5, "params": {"name": "sync_target", "arguments": {"target": "codex"}}}) + "\n").encode())
    proc.stdin.flush()
    start = time.time()
    line = proc.stdout.readline()
    elapsed = time.time() - start
    r = json.loads(line)
    assert "result" in r, "sync_target should return result"
    assert elapsed < 5.0, f"sync_target took {elapsed:.1f}s (>5s limit)"

    # Cleanup
    proc.stdin.close()
    proc.wait(timeout=5)
    print("ALL TESTS PASSED")
    ```

    Run this test and verify all 5 assertions pass. The sync_target call must complete within 5 seconds (success criteria #3).
    (Level 2: Proxy)
  </verify>
  <done>
    MCP server starts via `python src/mcp/server.py`, completes initialize handshake, lists 3 tools, executes get_status immediately, queues sync operations via worker thread with file-level locking, returns structured JSON results, handles concurrent requests gracefully (returns "busy" for in-progress syncs), and responds within 5 seconds for typical sync_target calls.
  </done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- Server starts without errors: `python src/mcp/server.py` (with stdin piped)
- Responds to initialize request with protocolVersion and capabilities
- tools/list returns exactly 3 tools (sync_all, sync_target, get_status)
- Each tool has valid inputSchema (json.loads succeeds)
- No output to stdout except JSON-RPC messages

Level 2 (Proxy):
- Full MCP handshake completes (initialize -> initialized -> tools/list -> tools/call)
- sync_target("codex") returns structured result within 5 seconds
- get_status returns immediately during in-progress sync
- Invalid tool arguments return isError=true (not protocol error)
- Second sync request while first is running returns "busy" response
- Server exits cleanly on stdin EOF

Level 3 (Deferred):
- Integration with Claude Code via plugin.json mcp.server entry
- External agent invocation test
- Production load testing with concurrent clients
</verification>

<success_criteria>
1. MCP server exposes sync_all, sync_target, get_status tools with JSON schema validation (MCP-01)
2. MCP server returns structured sync results with targets synced, items per target, errors with file paths and reasons (MCP-02)
3. sync_target("codex") completes within 5 seconds for typical project (success criteria #3)
4. Concurrent sync requests handled gracefully — second request returns immediate "busy" response, get_status always immediate (success criteria #4)
5. Server integrates with existing sync_lock for file-level mutual exclusion with /sync command and PostToolUse hook
6. All tool execution errors return isError=true in result (not JSON-RPC protocol errors)
</success_criteria>

<output>
After completion, create `.planning/phases/06-mcp-server-integration/06-02-SUMMARY.md`
</output>
