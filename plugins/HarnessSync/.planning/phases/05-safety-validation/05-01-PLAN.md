---
phase: 05-safety-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/backup_manager.py, src/symlink_cleaner.py]
autonomous: true
verification_level: proxy

must_haves:
  truths:
    - "BackupManager.backup_target() creates timestamped directory under ~/.harnesssync/backups/ with YYYYMMDD_HHMMSS format"
    - "BackupManager.rollback() restores backed-up files to original locations in LIFO order on sync failure"
    - "BackupManager.cleanup_old_backups() keeps exactly 10 most recent backups per target, deletes older ones"
    - "Backup preserves symlink structure (shutil.copytree with symlinks=True) — symlinks are not followed"
    - "SymlinkCleaner.cleanup() removes broken symlinks (is_symlink() and not exists()) from target directories"
    - "SymlinkCleaner scans .codex/skills/, .opencode/skills/, .opencode/agents/, .opencode/commands/ directories"
  artifacts:
    - path: "src/backup_manager.py"
      provides: "Pre-sync backup and rollback context manager"
      exports: ["BackupManager"]
    - path: "src/symlink_cleaner.py"
      provides: "Broken symlink detection and removal"
      exports: ["SymlinkCleaner"]
  key_links:
    - from: "src/backup_manager.py"
      to: "src/utils/paths.py"
      via: "import for harnesssync_dir path resolution"
      pattern: "from.*utils.*paths.*import"
    - from: "src/symlink_cleaner.py"
      to: "src/utils/logger.py"
      via: "import for cleanup logging"
      pattern: "from.*utils.*logger.*import"
---

<objective>
Implement pre-sync backup with rollback capabilities (SAF-01) and stale symlink cleanup (SAF-05).

Purpose: Protect target configurations from corruption during sync failures and clean up orphaned symlinks after sync operations. These are the file-system safety primitives that the orchestrator will invoke.

Output: Two independent utility modules — BackupManager for timestamped backup/rollback and SymlinkCleaner for broken symlink detection and removal.

Research basis: Rollback context pattern from Python rollback library, timestamped backup pattern from ISO 8601 best practices, broken symlink detection from Python pathlib documentation.
</objective>

<execution_context>
@${CLAUDE_PLUGIN_ROOT}/references/execute-plan.md
@${CLAUDE_PLUGIN_ROOT}/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-safety-validation/05-RESEARCH.md
@src/utils/paths.py
@src/utils/logger.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement BackupManager with timestamped backup and rollback</name>
  <files>src/backup_manager.py</files>
  <action>
    Create src/backup_manager.py with a BackupManager class implementing SAF-01.

    Based on the rollback context pattern (Python rollback library) and timestamped backup best practices:

    **BackupManager class:**
    - __init__(self, backup_root: Path = None): Default backup_root is ~/.harnesssync/backups/. Accept override for testing.
    - backup_target(self, target_path: Path, target_name: str) -> Path: Create timestamped backup of a target config file or directory. Timestamp format: YYYYMMDD_HHMMSS. Backup stored at backup_root/{target_name}/{filename}_{timestamp}/. Use shutil.copy2() for files (preserves metadata), shutil.copytree(symlinks=True) for directories (preserves symlinks — CRITICAL: do not follow symlinks per RESEARCH.md Pitfall 2). Create parent directories with exist_ok=True. Return backup path.
    - rollback(self, backups: list[tuple[Path, Path]]): Restore files from backup. Takes list of (backup_path, original_path) tuples. Process in LIFO order (reversed). For each: remove failed sync result (rmtree for dirs, unlink for files), then restore from backup (copytree with symlinks=True for dirs, copy2 for files). Wrap each restore in try/except — log errors but continue (best-effort rollback per RESEARCH.md).
    - cleanup_old_backups(self, target_name: str, keep_count: int = 10): Remove old backups for a target beyond keep_count. Get all backup directories for target, sort by mtime (newest first), delete entries beyond keep_count using shutil.rmtree(). Wrap rmtree in try/except per RESEARCH.md Pitfall (backup cleanup failures should not break sync).

    **BackupContext context manager (separate class or nested):**
    - __init__(self, backup_manager: BackupManager): Store reference to BackupManager.
    - register(self, backup_path: Path, original_path: Path): Append to internal _backups list.
    - __enter__(self): Return self.
    - __exit__(self, exc_type, exc_val, exc_tb): If exception occurred, call backup_manager.rollback(self._backups). Return False (don't suppress exception).

    Use imports: shutil, pathlib.Path, datetime, src.utils.paths (for harnesssync_dir), src.utils.logger.Logger.

    Avoid:
    - shutil.move() for atomic operations (use os.replace() per RESEARCH.md Don't Hand-Roll)
    - Backing up the backup directory itself (exclude backup_root from operations per RESEARCH.md anti-pattern)
    - Following symlinks during backup (always symlinks=True)
  </action>
  <verify>
    python3 -c "
import tempfile, os, sys
sys.path.insert(0, '.')
from pathlib import Path
from src.backup_manager import BackupManager, BackupContext

# Test 1: Backup creates timestamped directory
with tempfile.TemporaryDirectory() as tmpdir:
    backup_root = Path(tmpdir) / 'backups'
    target_file = Path(tmpdir) / 'test_config.json'
    target_file.write_text('{\"key\": \"value\"}')

    bm = BackupManager(backup_root=backup_root)
    bp = bm.backup_target(target_file, 'codex')
    assert bp.exists(), 'Backup not created'
    assert 'codex' in str(bp), 'Target name not in path'
    # Check timestamp format in path
    parts = bp.name.split('_')
    assert len(parts) >= 3, f'Unexpected backup name format: {bp.name}'
    print('PASS: Backup creates timestamped directory')

# Test 2: Backup preserves content
    backed_up = list(bp.rglob('*'))
    assert any(f.read_text() == '{\"key\": \"value\"}' for f in backed_up if f.is_file()), 'Content not preserved'
    print('PASS: Backup preserves content')

# Test 3: Rollback restores files
    target_file.write_text('CORRUPTED')
    bm.rollback([(bp, target_file)])
    # After rollback, original content should be restored
    print('PASS: Rollback executed without error')

# Test 4: Cleanup keeps only N backups
    for i in range(15):
        f = Path(tmpdir) / f'cfg_{i}.json'
        f.write_text(f'config {i}')
        bm.backup_target(f, 'test_target')
    bm.cleanup_old_backups('test_target', keep_count=10)
    remaining = list((backup_root / 'test_target').iterdir())
    assert len(remaining) <= 10, f'Expected <=10, got {len(remaining)}'
    print('PASS: Cleanup keeps <=10 backups')

# Test 5: BackupContext rollback on exception
    try:
        with BackupContext(bm) as ctx:
            bp2 = bm.backup_target(target_file, 'ctx_test')
            ctx.register(bp2, target_file)
            raise ValueError('Simulated failure')
    except ValueError:
        pass
    print('PASS: BackupContext handles exception')

print('All 5 backup tests passed')
" (Level 2: Proxy)
  </verify>
  <done>
    BackupManager creates timestamped backups under ~/.harnesssync/backups/{target_name}/, preserves symlink structure, restores in LIFO order on failure, and cleanup retains exactly keep_count most recent backups. BackupContext provides context manager for automatic rollback on exception.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement SymlinkCleaner for broken symlink removal</name>
  <files>src/symlink_cleaner.py</files>
  <action>
    Create src/symlink_cleaner.py implementing SAF-05.

    Based on pathlib documentation's broken symlink detection pattern (is_symlink() and not exists()):

    **SymlinkCleaner class:**
    - __init__(self, project_dir: Path): Store project directory for resolving target directories.
    - find_broken_symlinks(self, directory: Path) -> list[Path]: Scan directory recursively with rglob('*'). Return list of paths where path.is_symlink() is True AND path.exists() is False. Do NOT use path.lexists() — it returns True for broken links (per RESEARCH.md caveat). Handle non-existent directories gracefully (return empty list).
    - cleanup(self, target_name: str) -> list[Path]: Clean broken symlinks for a specific target. Map target_name to directories:
      - "codex": [project_dir/.codex/skills/]
      - "opencode": [project_dir/.opencode/skills/, project_dir/.opencode/agents/, project_dir/.opencode/commands/]
      - "gemini": [] (Gemini uses inline content, no symlinks)
    For each directory, call find_broken_symlinks(), then unlink() each broken symlink. Wrap unlink in try/except (log error, continue). Return list of all removed paths.
    - cleanup_all(self) -> dict[str, list[Path]]: Run cleanup for all targets. Return dict mapping target_name -> list of removed paths.

    Use Logger for reporting removed symlinks (info level) and errors (error level).

    Avoid:
    - Using os.path.exists() alone for detection (returns False for both broken links AND non-existent paths per RESEARCH.md)
    - Removing non-symlink files (only unlink items where is_symlink() is True)
    - Descending into broken symlink directories (rglob handles this correctly)
  </action>
  <verify>
    python3 -c "
import tempfile, os, sys
sys.path.insert(0, '.')
from pathlib import Path
from src.symlink_cleaner import SymlinkCleaner

# Test 1: Detect broken symlinks
with tempfile.TemporaryDirectory() as tmpdir:
    project = Path(tmpdir)
    skills_dir = project / '.codex' / 'skills'
    skills_dir.mkdir(parents=True)

    # Create a valid symlink
    real_file = project / 'real_skill.md'
    real_file.write_text('# Real Skill')
    valid_link = skills_dir / 'valid.md'
    valid_link.symlink_to(real_file)

    # Create a broken symlink
    broken_link = skills_dir / 'broken.md'
    broken_link.symlink_to(project / 'nonexistent.md')

    cleaner = SymlinkCleaner(project)
    broken = cleaner.find_broken_symlinks(skills_dir)
    assert len(broken) == 1, f'Expected 1 broken, got {len(broken)}'
    assert broken[0].name == 'broken.md', 'Wrong broken symlink detected'
    print('PASS: Detects broken symlinks')

# Test 2: Cleanup removes broken symlinks only
    removed = cleaner.cleanup('codex')
    assert len(removed) == 1, f'Expected 1 removed, got {len(removed)}'
    assert not broken_link.exists() and not broken_link.is_symlink(), 'Broken link not removed'
    assert valid_link.is_symlink() and valid_link.exists(), 'Valid link was removed!'
    print('PASS: Cleanup removes only broken symlinks')

# Test 3: Cleanup for opencode directories
    for d in ['.opencode/skills', '.opencode/agents', '.opencode/commands']:
        (project / d).mkdir(parents=True, exist_ok=True)
        link = project / d / 'broken.md'
        link.symlink_to(project / 'missing.md')

    removed = cleaner.cleanup('opencode')
    assert len(removed) == 3, f'Expected 3 removed, got {len(removed)}'
    print('PASS: OpenCode cleanup covers all directories')

# Test 4: Gemini returns empty (no symlinks)
    removed = cleaner.cleanup('gemini')
    assert len(removed) == 0, 'Gemini should have no symlinks to clean'
    print('PASS: Gemini returns empty list')

# Test 5: Non-existent directory handled gracefully
    broken = cleaner.find_broken_symlinks(project / 'nonexistent_dir')
    assert len(broken) == 0, 'Non-existent dir should return empty'
    print('PASS: Non-existent directory handled')

print('All 5 symlink cleaner tests passed')
" (Level 2: Proxy)
  </verify>
  <done>
    SymlinkCleaner correctly detects broken symlinks using is_symlink() + not exists() pattern, removes them from .codex/skills/, .opencode/skills/, .opencode/agents/, .opencode/commands/ directories, preserves valid symlinks, and handles non-existent directories gracefully. Gemini target correctly returns empty (no symlinks used).
  </done>
</task>

</tasks>

<verification>
Level 1 (Sanity):
- S1: BackupManager class exists with backup_target, rollback, cleanup_old_backups methods
- S2: Backup directory contains timestamped subdirectory with YYYYMMDD_HHMMSS format
- S3: SymlinkCleaner class exists with find_broken_symlinks, cleanup, cleanup_all methods
- S4: Broken symlink detection uses is_symlink() and not exists() pattern (grep verify)
- S5: shutil.copytree called with symlinks=True (grep verify)

Level 2 (Proxy):
- P1: Backup + restore round-trip preserves file content byte-for-byte
- P2: Cleanup retains exactly 10 backups when 15 exist
- P3: BackupContext triggers rollback on exception
- P4: SymlinkCleaner removes broken symlinks while preserving valid ones
- P5: OpenCode cleanup covers skills/, agents/, commands/ directories

Level 3 (Deferred):
- DEFER-05-01: Windows junction detection in SymlinkCleaner (requires Windows environment)
- DEFER-05-02: Large directory backup performance (requires >1GB test data)
</verification>

<success_criteria>
1. BackupManager creates timestamped backups, restores on failure, and maintains retention policy (keep 10)
2. BackupContext provides automatic rollback on exception via context manager pattern
3. SymlinkCleaner detects and removes broken symlinks across all target directories
4. All 10 verification tests pass (5 backup + 5 symlink)
5. No external dependencies added (Python stdlib only)
</success_criteria>

<output>
After completion, create `.planning/phases/05-safety-validation/05-01-SUMMARY.md`
</output>
